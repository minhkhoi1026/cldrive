//{"dim_in":4,"dim_vector_num":6,"input":1,"out_grad":2,"tmp":3,"vector_length":5,"vector_weight_grad":0}
int hook(int argId, int id) {
	int gID = get_global_id(0);
	printf("%d,%d,%d\n", gID, argId, id);
	return id;
}
inline float sigmoid(float z) {
  return 1.0f / (1.0f + exp(-z));
}
inline float sigmoid_gradient(float y) {
  return y * (1.0f - y);
}

inline float relu(float z) {
  return z > 0 ? z : 0;
}
inline float relu_gradient(float y) {
  return y > 0 ? 1 : 0;
}

inline float tanh_gradient(float y) {
  return 1.0f - y * y;
}

inline float softrelu(float z) {
  return log(1.0f + exp(z));
}
inline float softrelu_gradient(float y) {
  return 1.0f - exp(-y);
}

inline float leakyrelu(float z) {
  return z > 0 ? z : 0.25f * z;
}
inline float leakyrelu_gradient(float y) {
  return y > 0 ? 1 : 0.25f;
}

inline float linear_regression(float y, float label) {
  float delta = y - label;
  return delta * delta;
}
inline float linear_regression_gradient(float y, float label) {
  return y - label;
}

inline float negative_log_likelihood_gradient(float y, bool i_equal_j) {
  return i_equal_j ? y - 1.0f : y;
}

kernel void back_propagate_embedding(global float* vector_weight_grad, const global float* input, const global float* out_grad, local float* tmp, const int dim_in, const int vector_length, const int dim_vector_num) {
  const int GID = get_global_id(0);
  for (int i = 0; i < dim_in; i++)
    vector_weight_grad[hook(0, ((int)input[ihook(1, i)) * vector_length + GID)] += out_grad[hook(2, i * vector_length + GID)];
}